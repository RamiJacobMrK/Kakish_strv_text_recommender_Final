# Kakish_text_recommender_Final: Text Recommendation System

## Overview

I developed this project as a prototype text recommendation system for a client. It demonstrates how TF-IDF, SVD, and Annoy are utilized to tackle content-based filtering. Given an input text, the system will return a list of the most relevant entries from the dataset "sample_posts". Sample posts could be integrated into larger datasets, but for simplicity and time constraints, I kept it small for now.

## Technologies Used

* **FastAPI:** A modern, fast web framework for building APIs with Python.
* **scikit-learn:** Used for TF-IDF vectorization and Truncated SVD for dimensionality reduction.
* **Annoy:** Used for approximate nearest neighbor search, enabling efficient similarity search.
* **NLTK:** Used for text preprocessing (lemmatization and stop word removal).
* **Pandas:** Used for data loading and manipulation.
* **python-dotenv:** Used for managing environment variables.
* **Uvicorn:** An ASGI server used to run the FastAPI application.

## Project Structure

```
Kakish_text_recommender_Final/
├── data/
│   └── sample_posts.csv   <-- Input text data (CSV format)
├── models/                <-- Trained models (generated by build_model.py)  (not committed to Git)
├── venv/                  <-- Virtual environment (not committed to Git)
├── api.py                 <-- FastAPI application
├── build_model.py         <-- Model building script
├── data_processing.py     <-- Data loading and preprocessing
├── query.py               <-- Similarity search logic
├── .env                   <-- Environment variables
├── requirements.txt       <-- Project dependencies
└── README.md              <-- This file
```

## Setup and Installation

1. **Clone the repository:**

    ```bash
    git clone https://github.com/RamiJacobMrK/Kakish_text_recommender_Final.git
    cd Kakish_text_recommender_Final
    ```

2. **Create and activate a virtual environment:**

    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Linux/macOS
    .\venv\Scripts\activate  # On Windows (PowerShell)
    ```

3. **Install dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

4. **NLTK Data:** The necessary NLTK data (stopwords and wordnet) will be downloaded automatically the first time you run `data_processing.py` or `build_model.py`.

## Building the Models

To build the TF-IDF, SVD, and Annoy models, run:

```bash
python build_model.py
```

## Running the API

To start the FastAPI development server, run:

```bash
uvicorn api:app --reload
```

You can access the interactive API documentation (Swagger UI) at: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

## API Endpoints

* **`POST /recommend`:**
    * **Description:** Gets similar text content recommendations.
    * **Request Body (JSON):**
        ```json
        {
          "text": "your search query here, try STRV",
          "top_k": 5
        }
        ```
        * `text`: (required, string) The text query string.
        * `top_k`: (optional, integer, default: 5) The number of recommendations to return.
    * **Response Body (JSON):**
        ```json
        {
          "query": "your search query here",
          "results": [
            {
              "text": "...",
              "label": ...,
              "clean_text": "..."
            },
            ...
          ]
        }
        ```
        * `query`: The original query text.
        * `results`: A list of similar text entries, each with:
            * `text`: The original text from the dataset.
            * `label`: The label (if present in your CSV).
            * `clean_text`: The preprocessed text.

* **`GET /health`:**
    * **Description:** Checks the health of the service.
    * **Response (JSON):**
        ```json
        {
            "status": "OK",
            "components": {
                "text_model": True,
                "database": False
            }
        }
        ```

## Evaluation and Improvements

This prototype uses a small example dataset (`sample_posts.csv`). The quality of the recommendations is directly related to the size. While this version works, improving the dataset would improve the results.

**Explored Improvements (I have tried but did not submit due to time constraints and practicality):**
* **Sentence Transformers:** I explored replacing the TF-IDF + SVD approach with a pre-trained Sentence Transformer model (e.g., from the sentence-transformers library). I have the code and the model but due to time constraints and to keep the submission focused on the core requirements, I opted to submit what I have with this model.
* **Larger Dataset (20 Newsgroups):** I have tried using the 20 newsgroup dataset and other larger and more diverse text datasets to improve the model's generalization ability. However, due to time constraints and practicality, I did not submit it because this would have meant creating a whole new model and committing it to GitHub.
* **Image Recommendation:** I partially developed code for using a pre-trained ResNet50 model and Annoy for image similarity search, but it is not included in this submission.

**Potential Improvements:**

* **Larger Dataset:** Using a significantly larger and more representative dataset is the *most impactful* improvement.
* **Advanced Preprocessing:** Explore more sophisticated text cleaning.
* **TF-IDF Tuning:** Experiment with different TF-IDF parameters.
* **SVD Tuning:** Experiment with different numbers of components for SVD.
* **Annoy Tuning:** Experiment with the number of trees in the Annoy index.
* **Sentence Transformers:** Replace TF-IDF + SVD with a pre-trained Sentence Transformer model.
* **Database Integration:** Integrate a database.
* **User Feedback:** Incorporate user feedback.
* **Hybrid Approach:** Combine content-based filtering with collaborative filtering for more personalized recommendations.

## Deployment (Conceptual)

For a production deployment, you would typically follow these steps (this is a high-level overview):

1. **Cloud Provider:** Choose a cloud provider (AWS, Google Cloud, Azure, etc.).
2. **Containerization (Docker):** Package your application.
3. **Production ASGI Server:** Use Gunicorn or Uvicorn with multiple workers:

    ```bash
    gunicorn api:app -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000
    ```

4. **Environment Variables:** Securely set environment variables.
5. **Deploy:** Deploy the container.
6. **Load Balancer (Optional):** Use a load balancer.
7. **Monitoring:** Implement monitoring.

## Author

Rami Jacob Kakish - [https://github.com/RamiJacobMrK](https://github.com/RamiJacobMrK)
