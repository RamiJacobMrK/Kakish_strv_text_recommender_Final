# Kakish_strv_text_recommender_Final: Text Recommendation System

## Overview

I developed this project as a prototype text recommendation system built for a recruitment task for STRV. It demonstrates how TF-IDF, SVD, and Annoy are utilized to tackle content-based filtering. Given an input text, the system will then return a list of most relevant entries from the dataset "sample_posts" sample posts could be integrated to large datasets instead but for sake of simipliciy and time constraints I kept is so for now. 

## Technologies Used

*   **FastAPI:** A modern, fast web framework for building APIs with Python.
*   **scikit-learn:** Used for TF-IDF vectorization and Truncated SVD for dimensionality reduction.
*   **Annoy:** Used for approximate nearest neighbor search, enabling efficient similarity search.
*   **NLTK:** Used for text preprocessing (lemmatization and stop word removal).
*   **Pandas:** Used for data loading and manipulation.
*   **python-dotenv:** Used for managing environment variables.
*   **Uvicorn:** An ASGI server used to run the FastAPI application.

## Project Structure

Kakish_strv_text_recommender_Final/
├── data/
│   └── sample_posts.csv   <-- Input text data (CSV format)
├── models/                <-- Trained models (generated by build_model.py)
├── venv/                  <-- Virtual environment (not committed to Git)
├── api.py                 <-- FastAPI application
├── build_model.py        <-- Model building script
├── data_processing.py   <-- Data loading and preprocessing
├── query.py               <-- Similarity search logic
├── .env                   <-- Environment variables
├── requirements.txt       <-- Project dependencies
└── README.md              <-- This file

## Setup and Installation

1.  **Clone the repository:**

    ```bash
    git clone [https://github.com/RamiJacobMrK/Kakish_strv_text_recommender_Final.git](https://github.com/RamiJacobMrK/Kakish_strv_text_recommender_Final.git)
    cd Kakish_strv_text_recommender_Final
    ```

2.  **Create and activate a virtual environment:**

    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Linux/macOS
    .\venv\Scripts\activate  # On Windows (PowerShell)
    ```

3.  **Install dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

4.  **NLTK Data:** The necessary NLTK data (stopwords and wordnet) will be downloaded automatically the first time you run `data_processing.py` or `build_model.py`.

## Building the Models

To build the TF-IDF, SVD, and Annoy models, run:

```bash
python build_model.py

## Running the API

To start the FastAPI development server, run:

```bash
uvicorn api:app --reload
## API Endpoints

*   **`POST /recommend`:**
    *   **Description:** Gets similar text content recommendations.
    *   **Request Body (JSON):**
        ```json
        {
          "text": "your search query here",
          "top_k": 5
        }
        ```
        *   `text`: (required, string) The text query string.
        *   `top_k`: (optional, integer, default: 5) The number of recommendations to return.
    *   **Response Body (JSON):**
        ```json
        {
          "query": "your search query here",
          "results": [
            {
              "text": "...",
              "label": ...,
              "clean_text": "..."
            },
            ...
          ]
        }
        ```
        *   `query`: The original query text.
        *   `results`: A list of similar text entries, each with:
            *   `text`: The original text from the dataset.
            *   `label`: The label (if present in your CSV).
            *   `clean_text`: The preprocessed text.

*   **`GET /health`:**
    *   **Description:** Checks the health of the service.
    *   **Response (JSON):**
        ```json
        {
            "status": "OK",
            "components": {
                "text_model": True,
                "database": False
            }
        }
        ```
## Evaluation and Improvements

This prototype uses a small example dataset (`sample_posts.csv`). The quality of the recommendations is directly related to the size, diversity, and representativeness of the dataset.

**Potential Improvements:**

*   **Larger Dataset:** Using a significantly larger and more representative dataset is the *most impactful* improvement.
*   **Advanced Preprocessing:** Explore more sophisticated text cleaning.
*   **TF-IDF Tuning:** Experiment with different TF-IDF parameters.
*   **SVD Tuning:** Experiment with different numbers of components for SVD.
*   **Annoy Tuning:** Experiment with the number of trees in the Annoy index.
*   **Sentence Transformers:** Replace TF-IDF + SVD with a pre-trained Sentence Transformer model.
*   **Database Integration:** Integrate a database.
*   **User Feedback:** Incorporate user feedback.
## Deployment (Conceptual)

For a production deployment, you would typically follow these steps (this is a high-level overview):

1.  **Cloud Provider:** Choose a cloud provider.
2.  **Containerization (Docker):** Package your application (Dockerfile not required for this submission).
3.  **Production ASGI Server:** Use Gunicorn or Uvicorn with multiple workers:

    ```bash
    gunicorn api:app -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000
    ```

4.  **Environment Variables:** Securely set environment variables.
5.  **Deploy:** Deploy the container.
6.  **Load Balancer (Optional):** Use a load balancer.
7.  **Monitoring:** Implement monitoring.
Author
Rami Jacob Kakish - https://github.com/RamiJacobMrK